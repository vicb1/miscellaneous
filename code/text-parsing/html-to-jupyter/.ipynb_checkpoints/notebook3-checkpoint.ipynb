{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"The-Taxi-Problem\">The Taxi Problem<a class=\"anchor-link\" href=\"#The-Taxi-Problem\">¶</a></h2>There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is the taxi), and 4 destination locations.\n",
    "Actions: \n",
    "There are 6 discrete deterministic actions:\n",
    "<ul>\n",
    "<li>0: move south</li>\n",
    "<li>1: move north</li>\n",
    "<li>2: move east </li>\n",
    "<li>3: move west </li>\n",
    "<li>4: pickup passenger</li>\n",
    "<li>5: dropoff passenger</li>\n",
    "</ul>\n",
    "<p></p>\n",
    "Rewards: \n",
    "There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "Rendering:\n",
    "<ul>\n",
    "<li>blue: passenger</li>\n",
    "<li>magenta: destination</li>\n",
    "<li>yellow: empty taxi</li>\n",
    "<li>green: full taxi</li>\n",
    "<li>other letters: locations</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENV_NAME = \"Taxi-v2\"\n",
    "env = gym.make(ENV_NAME)\n",
    "env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Number of actions: %d\" % env.action_space.n)\n",
    "print(\"Number of states: %d\" % env.observation_space.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Keras-RL-and-gym's-discrete-environments\">Keras-RL and gym's discrete environments<a class=\"anchor-link\" href=\"#Keras-RL-and-gym's-discrete-environments\">¶</a></h2>Keras-RL <a href=\"https://github.com/keras-rl/keras-rl/tree/master/examples\">examples</a> does not use gym's discrete environment as examples. Being the beginner that I am to both Keras-RL and gym, I had to find another source to refer to for discrete environments. Therefore, I found and referred to <a href=\"https://gist.github.com/ceshine/eeb97564c21a77b8c315179f82b3fc08\">this example</a> which used gym's Frozen Lake environment, which is a discrete environment in gym, as reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"What-does-an-Embedding-layer-do-and-what-are-the-parameters?\">What does an Embedding layer do and what are the parameters?<a class=\"anchor-link\" href=\"#What-does-an-Embedding-layer-do-and-what-are-the-parameters?\">¶</a></h2><h4 id=\"Embedding(input_dimensions=500,-output_dimensions=6,-input_length)\"></h4><p></p>Embedding(input_dimensions=500, output_dimensions=6, input_length)<a class=\"anchor-link\" href=\"#Embedding(input_dimensions=500,-output_dimensions=6,-input_length)\">¶</a><p></p>In Deep Q-Learning, the input to the neural network are possible states of the environment and the output of the neural network is the action to be taken.\n",
    "The input_length for a discrete environment in OpenAi's gym (e.g Taxi, Frozen Lake) is 1 because the output from env.step(env.action_space.sample())[0] (e.g. the state it will be in), is a single number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.reset()\n",
    "env.step(env.action_space.sample())[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the Embedding layer, the input_dimensions refers to the number of states and output_dimensions refers to the vector space we are squishing it to. This means that we have 500 possible states and we want it to be represented by 6 values.\n",
    "<p></p>\n",
    "If you do not want to add any dense layers (meaning that you only want a single layer neural network, which is the embedding layer), you will have to set the output_dimensions of the Embedding layer to be the same as the action space of the environment. This means that output_dimensions must be 6 when you are using the Taxi environment because there can only be 6 actions, which are go up, go down, go left, go right, pickup passenger and drop passenger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_only_embedding = Sequential()\n",
    "model_only_embedding.add(Embedding(500, 6, input_length=1))\n",
    "model_only_embedding.add(Reshape((6,)))\n",
    "print(model_only_embedding.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you want to add Dense layers after the Embedding layer, you can choose your own output_dimensions for your Embedding layer (it does not have to follow the action space size), but the final Dense layer must have the same output size as your action space size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(500, 10, input_length=1))\n",
    "model.add(Reshape((10,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"What-does-the-Reshape-layer-do?\">What does the Reshape layer do?<a class=\"anchor-link\" href=\"#What-does-the-Reshape-layer-do?\">¶</a></h2>In the reshape layer, we take the output from the previous layer and reshape it to a rank 1 tensor (a one-dimensional array). In this notebook, (6,) means a one dimensional array with 6 values. For example, [1, 2, 3, 4, 5, 6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Parameters-when-fitting-the-neural-network\">Parameters when fitting the neural network<a class=\"anchor-link\" href=\"#Parameters-when-fitting-the-neural-network\">¶</a></h2>I tried to set the nb_steps and nb_max_episode_steps to be the same as total_episodes and max_steps in the previous blog post, <a href=\"https://tiewkh.github.io/blog/qlearning-openaitaxi/\">Q-learning with OpenAI Taxi.</a>\n",
    "<h3 id=\"I-will-be-training-both-the-neural-network-with-only-the-Embedding-layer-(dqn_only_embedding)-and-the-neural-network-with-a-few-Dense-layers-(dqn)\">I will be training both the neural network with only the Embedding layer (dqn_only_embedding) and the neural network with a few Dense layers (dqn)<a class=\"anchor-link\" href=\"#I-will-be-training-both-the-neural-network-with-only-the-Embedding-layer-(dqn_only_embedding)-and-the-neural-network-with-a-few-Dense-layers-(dqn)\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn_only_embedding = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=500, target_model_update=1e-2, policy=policy)\n",
    "dqn_only_embedding.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn_only_embedding.fit(env, nb_steps=1000000, visualize=False, verbose=1, nb_max_episode_steps=99, log_interval=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn_only_embedding.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=500, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1, nb_max_episode_steps=99, log_interval=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(\"Taxi-v2\"), overwrite=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A4_MDP_grim3_jup",
   "language": "python",
   "name": "a4_mdp_grim3_jup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
